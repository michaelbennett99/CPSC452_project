{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ad9083-0592-4a9a-b100-a1ab16328e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "True\n",
      "0.18.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2084df09-41dd-4ebd-865b-6a0d23fc4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pathlib import Path\n",
    "\n",
    "def listdir_nohidden(path: Path | str) -> list[str]:\n",
    "    \"\"\"\n",
    "    List files in a directory excluding hidden files.\n",
    "    \"\"\"\n",
    "    return [file for file in listdir(path) if not file.startswith(\".\")]\n",
    "\n",
    "def lazy_stoi(s: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert a string to an integer.\n",
    "    \"\"\"\n",
    "    return int(s) if s.replace(\",\", \"\").isdigit() else s\n",
    "\n",
    "def lazy_stof(s: str) -> float:\n",
    "    \"\"\"\n",
    "    Convert a string to a float.\n",
    "    \"\"\"\n",
    "    return float(s) if s.replace(\",\", \"\").replace(\".\", \"\", 1).isdigit() else s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0c90e6-3a23-4461-8d29-be7b774d7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from typing import Optional\n",
    "\n",
    "import gdown\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file_from_gdrive_gdown(\n",
    "        url: str,\n",
    "        file_name: Path | str,\n",
    "        overwrite: bool = False,\n",
    "        postprocess: callable = None\n",
    "    ):\n",
    "    print(f\"Downloading {url} to {file_name}\")\n",
    "    # check if file exists or has been unzipped\n",
    "    is_file = os.path.exists(file_name)\n",
    "    is_unzipped = os.path.exists(file_name.removesuffix('.zip'))\n",
    "    if not overwrite and (is_file or is_unzipped):\n",
    "        raise FileExistsError(f\"{file_name} exists.\")\n",
    "\n",
    "    # Make parent directory\n",
    "    if not os.path.isdir(os.path.dirname(file_name)):\n",
    "        os.makedirs(os.path.dirname(file_name))\n",
    "\n",
    "    gdown.download(url, output=str(file_name))\n",
    "\n",
    "    if postprocess:\n",
    "        try:\n",
    "            postprocess(file_name)\n",
    "        except Exception as e:\n",
    "            os.remove(file_name)\n",
    "            raise e\n",
    "\n",
    "def download_folder_from_gdrive_gdown(\n",
    "    url: str,\n",
    "    file_name: Path | str,\n",
    "    overwrite: bool = False,\n",
    "    postprocess: callable = None\n",
    "):\n",
    "    print(f\"Downloading {url} to {file_name}\")\n",
    "    # Check if folder exists\n",
    "    if not overwrite and os.path.isdir(file_name) and listdir_nohidden(file_name):\n",
    "        raise FileExistsError(f\"{file_name} exists and is not empty.\")\n",
    "\n",
    "    # Check if folder exists as a file\n",
    "    if not overwrite and os.path.exists(file_name):\n",
    "        raise FileExistsError(f\"{file_name} exists and is not a directory.\")\n",
    "\n",
    "    # Make parent directory\n",
    "    if not os.path.isdir(file_name):\n",
    "        os.makedirs(file_name)\n",
    "\n",
    "    gdown.download_folder(url, output=str(file_name))\n",
    "\n",
    "    if postprocess:\n",
    "        try:\n",
    "            postprocess(file_name)\n",
    "        except Exception as e:\n",
    "            os.removedirs(file_name)\n",
    "            raise e\n",
    "\n",
    "def download_file_from_dropbox(\n",
    "    url: str, file_name: Path | str, postprocess: Optional[callable] = None,\n",
    "    dry_run: bool = False\n",
    "):\n",
    "    print(f\"Downloading {url} to {file_name}.\")\n",
    "\n",
    "    if dry_run:\n",
    "        return\n",
    "\n",
    "    # Make parent directory\n",
    "    parent = os.path.dirname(file_name)\n",
    "    if not os.path.isdir(parent):\n",
    "        os.makedirs(parent)\n",
    "\n",
    "    # headers = {'user-agent': 'Wget/1.16 (linux-gnu)'}\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    total_size = int(r.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "\n",
    "    with tqdm(total=total_size, unit='B', unit_scale=True) as t:\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=block_size):\n",
    "                if chunk:\n",
    "                    t.update(len(chunk))\n",
    "                    f.write(chunk)\n",
    "\n",
    "    if postprocess:\n",
    "        try:\n",
    "            postprocess(file_name)\n",
    "        except Exception as e:\n",
    "            os.remove(file_name)\n",
    "            raise e\n",
    "\n",
    "def unzip_contents(path: Path | str):\n",
    "    # Check if we are directly given a zip file\n",
    "    if os.path.exists(path) and os.path.splitext(path)[1] == \".zip\":\n",
    "        print(f\"Unzipping {path}\")\n",
    "        with ZipFile(path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(os.path.dirname(path))\n",
    "        os.remove(path)\n",
    "\n",
    "        unzip_contents(path.removesuffix(\".zip\"))\n",
    "\n",
    "    # Otherwise, search directory for zip files\n",
    "    elif os.path.isdir(path):\n",
    "        print(f\"Searching {path} for zip files...\")\n",
    "        for d in tqdm(listdir_nohidden(path), desc=f\"Unzipping {os.path.basename(path)}\"):\n",
    "            zip_path = os.path.join(path, d)\n",
    "            if zip_path.endswith(\".zip\"):\n",
    "                print(f\"Unzipping {d}\")\n",
    "                with ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                    zip_ref.extractall(path)\n",
    "                os.remove(zip_path)\n",
    "\n",
    "            # Recursively unzip the contents\n",
    "                unzip_contents(path.removesuffix(\".zip\"))\n",
    "            elif os.path.isdir(zip_path):\n",
    "                unzip_contents(zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2144742-b05c-4555-977d-f1ea12b77c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from enum import Enum, auto\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import verify_str_arg\n",
    "from torch import tensor, Tensor\n",
    "from torchvision.tv_tensors._bounding_boxes import BoundingBoxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "# Download links for the DOTA dataset\n",
    "\n",
    "DOTA_TRAIN_HBB_URL = (\n",
    "    \"https://www.dropbox.com/scl/fi/k5e9wdfdu4qppyz283nss/\"\n",
    "    \"train_hbb.zip?rlkey=wrlr3fpqk8x02r5xzph8uuedk&st=kqaeuv3x&dl=1\"\n",
    ")\n",
    "DOTA_VAL_HBB_URL = (\n",
    "    \"https://www.dropbox.com/scl/fi/1820a7bhat8b5esv73u6h/\"\n",
    "    \"val_hbb.zip?rlkey=0ekae5kjspsq68cbkww8qjkl3&st=3z3ij651&dl=1\"\n",
    ")\n",
    "\n",
    "DOTA_TRAIN_OBB_URL = (\n",
    "    \"https://www.dropbox.com/scl/fi/zu3p9wqzlu86v0kuu4v0p/\"\n",
    "    \"train.zip?rlkey=bwih2x8xd3zpldj7l1s4owy9a&st=zrisaio3&dl=1\"\n",
    ")\n",
    "DOTA_VAL_OBB_URL = (\n",
    "    \"https://www.dropbox.com/scl/fi/k3d45d22iz1op3gifazw4/\"\n",
    "    \"val.zip?rlkey=zv7fgnkf3yqztj93cztzsfsgd&st=ghtu1ntz&dl=1\"\n",
    ")\n",
    "\n",
    "# Dataset defaults\n",
    "\n",
    "DEFAULT_DOTA_PATH = Path(os.getcwd()) / \"data\" / \"dota\"\n",
    "\n",
    "DATASET_DICT = {\n",
    "    (\"train\", \"hbb\"): {\n",
    "        \"url\": DOTA_TRAIN_HBB_URL,\n",
    "        \"base_dir\": os.path.join(\"hbb\", \"train_hbb\")  # Adjusted to the correct folder name\n",
    "    },\n",
    "    (\"val\", \"hbb\"): {\n",
    "        \"url\": DOTA_VAL_HBB_URL,\n",
    "        \"base_dir\": os.path.join(\"hbb\", \"val_hbb\")  # Make sure this is correct\n",
    "    },\n",
    "    (\"train\", \"obb\"): {\n",
    "        \"url\": DOTA_TRAIN_OBB_URL,\n",
    "        \"base_dir\": os.path.join(\"obb\", \"train\")\n",
    "    },\n",
    "    (\"val\", \"obb\"): {\n",
    "        \"url\": DOTA_VAL_OBB_URL,\n",
    "        \"base_dir\": os.path.join(\"obb\", \"val\") \n",
    "    }\n",
    "}\n",
    "\n",
    "IMAGES_DIRNAME = \"images\"\n",
    "LABELS_DIRNAME = \"labels\"\n",
    "\n",
    "class Label(Enum):\n",
    "    \"\"\"\n",
    "    Enum class for the labels in the DOTA dataset. Automatically assigns\n",
    "    integers to each label.\n",
    "    \"\"\"\n",
    "    large_vehicle = auto()\n",
    "    small_vehicle = auto()\n",
    "    plane = auto()\n",
    "    ship = auto()\n",
    "    storage_tank = auto()\n",
    "    baseball_diamond = auto()\n",
    "    tennis_court = auto()\n",
    "    basketball_court = auto()\n",
    "    ground_track_field = auto()\n",
    "    harbor = auto()\n",
    "    bridge = auto()\n",
    "    helicopter = auto()\n",
    "    roundabout = auto()\n",
    "    soccer_ball_field = auto()\n",
    "    swimming_pool = auto()\n",
    "    container_crane = auto()\n",
    "    airport = auto()\n",
    "    helipad = auto()\n",
    "\n",
    "class Target(dict):\n",
    "    \"\"\"\n",
    "    Dict subclass that mostly acts the same but has a few extra methods for\n",
    "    convenience and overwrites __str__ and __repr__ to give a more informative\n",
    "    string representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, boxes = [], labels = [], difficult = [],  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self[\"boxes\"] = boxes\n",
    "        self[\"labels\"] = labels\n",
    "        self[\"difficult\"] = difficult\n",
    "\n",
    "    def __str__(self):\n",
    "        return json.dumps(\n",
    "            {\n",
    "                \"attributes\": [\n",
    "                    k for k in self.keys()\n",
    "                    if k not in [\"boxes\", \"labels\", \"difficult\"]\n",
    "                ],\n",
    "                \"features\": [\"boxes\", \"labels\", \"difficult\"],\n",
    "                \"n_features\": len(self['boxes'])\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def add_attribute(self, key: str, value: str | int | float):\n",
    "        self[key] = value\n",
    "\n",
    "    def add_box(self, box: list[float], label: str, difficult: int):\n",
    "        self[\"boxes\"].append(box)\n",
    "        self[\"labels\"].append(label)\n",
    "        self[\"difficult\"].append(difficult)\n",
    "\n",
    "class DOTA(VisionDataset):\n",
    "    \"\"\"\n",
    "    Class for the DOTA dataset. The dataset is split into train and val\n",
    "    sets, and the annotations can be either horizontal bounding boxes (hbb)\n",
    "    or oriented bounding boxes (obb).\n",
    "\n",
    "    Once downloaded, indexing the dataset will return a tuple of an image\n",
    "    and a target dictionary. The target dictionary contains the following\n",
    "    keys:\n",
    "        - boxes: A BoundingBoxes object containing the bounding boxes.\n",
    "        - labels: A tensor containing the labels for each bounding box.\n",
    "        - difficult: A tensor containing the difficulty of each bounding box.\n",
    "    and any other annotation keys that are present in the target file.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Path | str = DEFAULT_DOTA_PATH,\n",
    "        split: str = \"train\",\n",
    "        annotation_type: Optional[str] = \"hbb\",\n",
    "        to_tensor: Optional[bool] = True,\n",
    "        transforms: Optional[callable] = None,\n",
    "        download: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the DOTA dataset. Gives you either the train or val split\n",
    "        with either horizontal bounding boxes (hbb) or oriented bounding boxes\n",
    "        and optional conversion to tensors (required for transforms).\n",
    "\n",
    "        Args:\n",
    "            root (Path | str): The root directory to save the dataset. Defaults\n",
    "                to <current working directory>/data/dota.\n",
    "            split (str): The split of the dataset to use. Either \"train\" or\n",
    "                \"val\". Defaults to \"train\".\n",
    "            annotation_type (str): The type of annotation to use. Either \"hbb\"\n",
    "                or \"obb\". Defaults to \"hbb\".\n",
    "            to_tensor (bool): Whether to convert the images and targets to\n",
    "                tensors. Defaults to True.\n",
    "            transforms (callable): A callable transform to apply to the images\n",
    "                and targets. The transform should take in an image and a target\n",
    "                dictionary and return the transformed image and target. Defaults\n",
    "                to None. If not none, to_tensor is set to True.\n",
    "            download (bool): Whether to download the dataset if it doesn't\n",
    "                exist. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms)\n",
    "\n",
    "        self.split = verify_str_arg(split, \"split\", (\"train\", \"val\"))\n",
    "        self.annotation_type = verify_str_arg(\n",
    "            annotation_type, \"annotation_type\", (\"hbb\", \"obb\")\n",
    "        )\n",
    "\n",
    "        # Transforms require the targets to be tensors\n",
    "        self.to_tensor = to_tensor\n",
    "        if transforms:\n",
    "            self.to_tensor = True\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Cannot do transforms or tensors for obb annotations\n",
    "        if self.annotation_type == \"obb\":\n",
    "            self.to_tensor = False\n",
    "            self.transforms = None\n",
    "\n",
    "        # Set up the download\n",
    "        dataset_dict = DATASET_DICT[(self.split, self.annotation_type)]\n",
    "\n",
    "        self.url = dataset_dict[\"url\"]\n",
    "        file_path = Path(root) / dataset_dict[\"base_dir\"]\n",
    "\n",
    "        if download:\n",
    "            self.download(\n",
    "                self.url, file_path, postprocess=self.unzip_contents\n",
    "            )\n",
    "        if not self.val_files(file_path):\n",
    "            msg = (\n",
    "                \"The files in the directory are not in the correct format. \",\n",
    "                \"Consider setting download=True to download the files.\"\n",
    "            )\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        image_dir = file_path / IMAGES_DIRNAME\n",
    "        label_dir = file_path / LABELS_DIRNAME\n",
    "        self.images = sorted([\n",
    "            os.path.join(image_dir, img) for img in listdir_nohidden(image_dir)\n",
    "        ])\n",
    "        self.targets = sorted([\n",
    "            os.path.join(label_dir, tgt) for tgt in listdir_nohidden(label_dir)\n",
    "        ])\n",
    "\n",
    "        if len(self.images) != len(self.targets):\n",
    "            raise ValueError(\n",
    "                \"Number of images and labels do not match.\",\n",
    "                f\"Images: {len(self.images)}\",\n",
    "                f\"Labels: {len(self.targets)}\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and label\n",
    "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        target = self.parse_dota_targets(self.targets[idx])\n",
    "\n",
    "        # Convert to tensors\n",
    "        if self.to_tensor:\n",
    "            img, target = self.to_tensors(img, target)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "    @staticmethod\n",
    "    def to_tensors(img: Image, target: Target) -> tuple[Tensor, Target]:\n",
    "        \"\"\"\n",
    "        Convert images and targets to tensors.\n",
    "        \"\"\"\n",
    "        img = F.to_tensor(img)\n",
    "        target = dict(\n",
    "            boxes=BoundingBoxes(\n",
    "                target[\"boxes\"],\n",
    "                format=\"XYXY\",\n",
    "                canvas_size=img.shape[1:],\n",
    "                dtype=img.dtype,\n",
    "                device=img.device\n",
    "            ),\n",
    "            labels=tensor([x.value for x  in target[\"labels\"]]),\n",
    "            difficult=tensor(target[\"difficult\"])\n",
    "        )\n",
    "        return img, target\n",
    "\n",
    "    def draw_bounding_boxes(self, idx: int, width: int = 5) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Draw the bounding boxes on the image at index idx with the given width.\n",
    "        \"\"\"\n",
    "        img, target = self.__getitem__(idx)\n",
    "\n",
    "        # guarantee we get tensors\n",
    "        if not self.to_tensor:\n",
    "            img, target = self.to_tensors(img, target)\n",
    "\n",
    "        uint8_img = (img * 255).to(torch.uint8)\n",
    "        return F.to_pil_image(\n",
    "            draw_bounding_boxes(uint8_img, target[\"boxes\"], width=width)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def val_files(file_path: Path) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that the necessary files are in the correct structure.\n",
    "        \n",
    "        Args:\n",
    "            file_path (Path): The directory path to validate.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the validation is successful, False otherwise.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(file_path):\n",
    "            print(\"Directory does not exist:\", file_path)\n",
    "            return False\n",
    "    \n",
    "        image_dir = file_path / IMAGES_DIRNAME\n",
    "        label_dir = file_path / LABELS_DIRNAME\n",
    "    \n",
    "        # Check directories exist\n",
    "        if not os.path.isdir(image_dir) or not os.path.isdir(label_dir):\n",
    "            print(\"Required subdirectories do not exist.\")\n",
    "            return False\n",
    "    \n",
    "        # Check that all files in images have a corresponding file in labels\n",
    "        image_files = {os.path.splitext(f)[0]: f for f in os.listdir(image_dir) if f.endswith(\".png\")}\n",
    "        label_files = {os.path.splitext(f)[0]: f for f in os.listdir(label_dir) if f.endswith(\".txt\")}\n",
    "    \n",
    "        if not image_files:\n",
    "            print(\"No image files found in\", image_dir)\n",
    "            return False\n",
    "    \n",
    "        if not label_files:\n",
    "            print(\"No label files found in\", label_dir)\n",
    "            return False\n",
    "    \n",
    "        # Ensure each image has a corresponding label file\n",
    "        for base_name in image_files:\n",
    "            if base_name not in label_files:\n",
    "                print(\"Missing label for image:\", base_name)\n",
    "                return False\n",
    "    \n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def unzip_contents(path: Path | str, delete: Optional[bool] = True):\n",
    "        \"\"\"\n",
    "        Directly unzip a single zip file and into a directory with the same name\n",
    "        \"\"\"\n",
    "        dir_path = Path(path).parent\n",
    "        if os.path.isfile(path) and os.path.splitext(path)[1] == \".zip\":\n",
    "            print(f\"Unzipping {path}\")\n",
    "            with ZipFile(path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(dir_path)\n",
    "            zip_dir = dir_path / listdir_nohidden(dir_path)[0]\n",
    "            os.rename(zip_dir, path.removesuffix(\".zip\"))\n",
    "            if delete:\n",
    "                os.remove(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def download(url: str, path: Path | str, **kwargs: Optional[dict]):\n",
    "        \"\"\"\n",
    "        Download the zip file from dropbox and unzip it into path if path\n",
    "        doesn't have the correct directory structure.\n",
    "        \"\"\"\n",
    "        if not DOTA.val_files(path):\n",
    "            download_file_from_dropbox(\n",
    "                url, f\"{path}.zip\", **kwargs\n",
    "            )\n",
    "\n",
    "    def parse_dota_targets(self, target: str) -> Target:\n",
    "        \"\"\"\n",
    "        Parse the DOTA target file into a Target object.\n",
    "        \"\"\"\n",
    "        res = Target()\n",
    "        with open(target, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                ws_split = line.split()\n",
    "                if len(ws_split) == 1:\n",
    "                    l0, l1 = ws_split[0].split(\":\")\n",
    "                    res.add_attribute(lazy_stof(l0), lazy_stof(l1))\n",
    "                elif len(ws_split) == 10:\n",
    "                    if self.annotation_type == \"hbb\":\n",
    "                        # if annotation type is hbb, then the box is in the form\n",
    "                        # [xmin, ymin, xmax, ymin, xmax, ymax, xmin, ymax]\n",
    "                        box = [float(ws_split[i]) for i in [0, 1, 2, 5]]\n",
    "                    else:\n",
    "                        box = [float(x) for x in ws_split[:-2]]\n",
    "                    label = ws_split[-2].replace(\"-\", \"_\")\n",
    "                    label = getattr(Label, label)\n",
    "                    difficult = int(ws_split[-1])\n",
    "                    res.add_box(box, label, difficult)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22b5ea5a-7896-4041-9272-df27c7d1b589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=19, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=76, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def create_model():\n",
    "    num_classes = len(Label) + 1  # +1 for the background class\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Replace the classifier with a new one that has the right number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "# Setup device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19324b91-025d-4002-af3e-04bbd23dee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomResizedCrop(300),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomRotation(180),\n",
    "    v2.SanitizeBoundingBoxes(),\n",
    "    v2.ToDtype(torch.float32)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d827868-b105-43bb-93ac-fcd7df8b6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "train_dataset = DOTA(split='train', annotation_type='hbb', transforms=transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76d72ad5-60bb-4e68-89d1-c59bf5c54024",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = DOTA(split='val', annotation_type='hbb', transforms=transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62e60385-9bb6-466a-ab31-5de3b340ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training Loop\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in data_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            running_loss += losses.item()\n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "719612d1-0f41-4ab9-a10d-24bded1b540b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.89 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, optimizer, val_loader, device)\n\u001b[0;32m      5\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, val_loader, device)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 16\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m images \u001b[38;5;241m=\u001b[39m [image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m     14\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m---> 16\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n\u001b[0;32m     17\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:379\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 379\u001b[0m labels, matched_gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_targets_to_anchors(anchors, targets)\n\u001b[0;32m    380\u001b[0m regression_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[0;32m    381\u001b[0m loss_objectness, loss_rpn_box_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[0;32m    382\u001b[0m     objectness, pred_bbox_deltas, labels, regression_targets\n\u001b[0;32m    383\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:208\u001b[0m, in \u001b[0;36mRegionProposalNetwork.assign_targets_to_anchors\u001b[1;34m(self, anchors, targets)\u001b[0m\n\u001b[0;32m    206\u001b[0m     labels_per_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((anchors_per_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     match_quality_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_similarity(gt_boxes, anchors_per_image)\n\u001b[0;32m    209\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_matcher(match_quality_matrix)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# get the targets corresponding GT for each proposal\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# NB: need to clamp the indices because we can have a single\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# GT in the image, and matched_idxs can be -2, which goes\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# out of bounds\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\ops\\boxes.py:287\u001b[0m, in \u001b[0;36mbox_iou\u001b[1;34m(boxes1, boxes2)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    286\u001b[0m     _log_api_usage_once(box_iou)\n\u001b[1;32m--> 287\u001b[0m inter, union \u001b[38;5;241m=\u001b[39m _box_inter_union(boxes1, boxes2)\n\u001b[0;32m    288\u001b[0m iou \u001b[38;5;241m=\u001b[39m inter \u001b[38;5;241m/\u001b[39m union\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iou\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\ops\\boxes.py:261\u001b[0m, in \u001b[0;36m_box_inter_union\u001b[1;34m(boxes1, boxes2)\u001b[0m\n\u001b[0;32m    258\u001b[0m area2 \u001b[38;5;241m=\u001b[39m box_area(boxes2)\n\u001b[0;32m    260\u001b[0m lt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, :\u001b[38;5;241m2\u001b[39m], boxes2[:, :\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m rb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m2\u001b[39m:], boxes2[:, \u001b[38;5;241m2\u001b[39m:])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m    263\u001b[0m wh \u001b[38;5;241m=\u001b[39m _upcast(rb \u001b[38;5;241m-\u001b[39m lt)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m    264\u001b[0m inter \u001b[38;5;241m=\u001b[39m wh[:, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m wh[:, :, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# [N,M]\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.89 GiB. GPU "
     ]
    }
   ],
   "source": [
    "# Main training and validation loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, val_loader, device)\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}, Training loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82569a21-66d8-488c-b192-06de17ecb7b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.79 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m [image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m      7\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m----> 9\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n\u001b[0;32m     10\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[0;32m     12\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:379\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 379\u001b[0m labels, matched_gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_targets_to_anchors(anchors, targets)\n\u001b[0;32m    380\u001b[0m regression_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[0;32m    381\u001b[0m loss_objectness, loss_rpn_box_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[0;32m    382\u001b[0m     objectness, pred_bbox_deltas, labels, regression_targets\n\u001b[0;32m    383\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:208\u001b[0m, in \u001b[0;36mRegionProposalNetwork.assign_targets_to_anchors\u001b[1;34m(self, anchors, targets)\u001b[0m\n\u001b[0;32m    206\u001b[0m     labels_per_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((anchors_per_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     match_quality_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_similarity(gt_boxes, anchors_per_image)\n\u001b[0;32m    209\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_matcher(match_quality_matrix)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# get the targets corresponding GT for each proposal\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# NB: need to clamp the indices because we can have a single\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# GT in the image, and matched_idxs can be -2, which goes\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# out of bounds\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\ops\\boxes.py:287\u001b[0m, in \u001b[0;36mbox_iou\u001b[1;34m(boxes1, boxes2)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    286\u001b[0m     _log_api_usage_once(box_iou)\n\u001b[1;32m--> 287\u001b[0m inter, union \u001b[38;5;241m=\u001b[39m _box_inter_union(boxes1, boxes2)\n\u001b[0;32m    288\u001b[0m iou \u001b[38;5;241m=\u001b[39m inter \u001b[38;5;241m/\u001b[39m union\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iou\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hw3\\Lib\\site-packages\\torchvision\\ops\\boxes.py:260\u001b[0m, in \u001b[0;36m_box_inter_union\u001b[1;34m(boxes1, boxes2)\u001b[0m\n\u001b[0;32m    257\u001b[0m area1 \u001b[38;5;241m=\u001b[39m box_area(boxes1)\n\u001b[0;32m    258\u001b[0m area2 \u001b[38;5;241m=\u001b[39m box_area(boxes2)\n\u001b[1;32m--> 260\u001b[0m lt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, :\u001b[38;5;241m2\u001b[39m], boxes2[:, :\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m    261\u001b[0m rb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m2\u001b[39m:], boxes2[:, \u001b[38;5;241m2\u001b[39m:])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m    263\u001b[0m wh \u001b[38;5;241m=\u001b[39m _upcast(rb \u001b[38;5;241m-\u001b[39m lt)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.79 GiB. GPU "
     ]
    }
   ],
   "source": [
    "accumulation_steps = 4  # Accumulate gradients over 4 forward passes\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.zero_grad()\n",
    "    for i, (images, targets) in enumerate(val_loader):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values()) / accumulation_steps\n",
    "\n",
    "        losses.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6221de0-e878-40db-ad53-72098faca40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 28           |        cudaMalloc retries: 33        |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  10637 MiB |  14906 MiB | 432291 MiB | 421654 MiB |\\n|       from large pool |  10548 MiB |  14812 MiB | 430424 MiB | 419876 MiB |\\n|       from small pool |     88 MiB |    113 MiB |   1867 MiB |   1778 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  10637 MiB |  14906 MiB | 432291 MiB | 421654 MiB |\\n|       from large pool |  10548 MiB |  14812 MiB | 430424 MiB | 419876 MiB |\\n|       from small pool |     88 MiB |    113 MiB |   1867 MiB |   1778 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  10611 MiB |  14879 MiB | 431620 MiB | 421009 MiB |\\n|       from large pool |  10522 MiB |  14784 MiB | 429757 MiB | 419234 MiB |\\n|       from small pool |     88 MiB |    113 MiB |   1863 MiB |   1774 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  14520 MiB |  14958 MiB |  49376 MiB |  34856 MiB |\\n|       from large pool |  14410 MiB |  14846 MiB |  49182 MiB |  34772 MiB |\\n|       from small pool |    110 MiB |    120 MiB |    194 MiB |     84 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   3882 MiB |   9444 MiB | 337541 MiB | 333658 MiB |\\n|       from large pool |   3861 MiB |   9421 MiB | 335505 MiB | 331644 MiB |\\n|       from small pool |     21 MiB |     25 MiB |   2035 MiB |   2014 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1187    |    1430    |   33758    |   32571    |\\n|       from large pool |     315    |     440    |   11612    |   11297    |\\n|       from small pool |     872    |     990    |   22146    |   21274    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1187    |    1430    |   33758    |   32571    |\\n|       from large pool |     315    |     440    |   11612    |   11297    |\\n|       from small pool |     872    |     990    |   22146    |   21274    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      88    |      96    |     157    |      69    |\\n|       from large pool |      33    |      40    |      60    |      27    |\\n|       from small pool |      55    |      60    |      97    |      42    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      67    |     116    |   16093    |   16026    |\\n|       from large pool |      24    |      59    |    6536    |    6512    |\\n|       from small pool |      43    |      63    |    9557    |    9514    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9ec9a-a287-4e5d-be78-e789f0016b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee3727-23c6-4663-aa5c-42ec9fdcbff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5d329-f2a8-41f0-bbd1-a7c4fcaed750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
